{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ASR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "local ASR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/suyash/Desktop/homegrownASR/.conda/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "# Initialization\n",
    "# insanely fast whisper Initialization\n",
    "import torch\n",
    "from transformers import pipeline\n",
    "from transformers.utils import is_flash_attn_2_available\n",
    "\n",
    "pipe = pipeline(\n",
    "    \"automatic-speech-recognition\",\n",
    "    model=\"openai/whisper-small\", # select checkpoint from https://huggingface.co/openai/whisper-large-v3#model-details\n",
    "    torch_dtype=torch.float16,\n",
    "    device=\"cuda:0\", # or mps for Mac devices\n",
    "    model_kwargs={\"attn_implementation\": \"flash_attention_2\"} if is_flash_attn_2_available() else {\"attn_implementation\": \"sdpa\"},\n",
    "    generate_kwargs = {\"language\":\"<|en|>\",\"task\": \"transcribe\"},\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialization pyaudio for microphone and Webrtc vad\n",
    "\n",
    "import pyaudio\n",
    "import webrtcvad\n",
    "import numpy as np\n",
    "from pydub import AudioSegment\n",
    "from io import BytesIO\n",
    "\n",
    "import time\n",
    "\n",
    "\n",
    "# Constants\n",
    "FORMAT = pyaudio.paInt16\n",
    "CHANNELS = 1\n",
    "RATE = 16000\n",
    "CHUNK_DURATION_MS = 30  # milliseconds\n",
    "CHUNK_SIZE = int(RATE * CHUNK_DURATION_MS / 1000)  # samples per chunk\n",
    "SILENCE_THRESHOLD = 100  # adjust this threshold according to your environment\n",
    "TARGET_DURATION_MS = 500  # milliseconds\n",
    "\n",
    "def is_silence(chunk):\n",
    "    return not vad.is_speech(chunk, RATE)\n",
    "\n",
    "# Initialize PyAudio\n",
    "audio = pyaudio.PyAudio()\n",
    "\n",
    "# Initialize WebRTC VAD\n",
    "vad = webrtcvad.Vad()\n",
    "vad.set_mode(3)  # Aggressive mode for better voice detection\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open stream\n",
    "stream = audio.open(format=FORMAT, channels=CHANNELS, rate=RATE,\n",
    "                     input=True, frames_per_buffer=CHUNK_SIZE)\n",
    "\n",
    "print(\"Listening...\")\n",
    "\n",
    "try:\n",
    "    accumulated_data = b\"\"\n",
    "    accumulated_silence = 0\n",
    "    while True:\n",
    "        chunk = stream.read(CHUNK_SIZE) #read mic data\n",
    "        is_silent = is_silence(chunk)\n",
    "        if is_silent:\n",
    "            accumulated_silence += CHUNK_DURATION_MS\n",
    "            if accumulated_silence >= TARGET_DURATION_MS:\n",
    "\n",
    "                if accumulated_data != b\"\":\n",
    "                    t0 = time.time()\n",
    "\n",
    "                    audio_stream = BytesIO(accumulated_data)\n",
    "\n",
    "                    audio_segment = AudioSegment.from_file(audio_stream, format='raw',\n",
    "                                        frame_rate=16000, channels=1, sample_width=2)\n",
    "                    audio_segment.export('output.wav', format='wav')\n",
    "                    t1 = time.time()\n",
    "\n",
    "                    outputs = pipe( \"output.wav\", chunk_length_s=30,batch_size=24,return_timestamps=False,)\n",
    "                    t2 = time.time()\n",
    "\n",
    "                    print(outputs)\n",
    "                    print(t1-t0)\n",
    "                    print(t2-t1)\n",
    "                # segments, info = model.transcribe('output.wav', beam_size=5)\n",
    "                # print(segments)\n",
    "\n",
    "                    accumulated_data=b\"\"\n",
    "\n",
    "\n",
    "                #print(\"No sound detected for {} milliseconds\".format(accumulated_silence))\n",
    "                accumulated_silence = 0  # Reset accumulated silence after printing message\n",
    "        else:\n",
    "            accumulated_data += chunk\n",
    "            accumulated_silence = 0  # Reset accumulated silence if sound is detected\n",
    "except KeyboardInterrupt:\n",
    "    pass\n",
    "\n",
    "# Cleanup\n",
    "print(\"Stopping...\")\n",
    "stream.stop_stream()\n",
    "stream.close()\n",
    "audio.terminate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Open ai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv() \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "client = OpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialization pyaudio for microphone and Webrtc vad\n",
    "\n",
    "import pyaudio\n",
    "import webrtcvad\n",
    "import numpy as np\n",
    "from pydub import AudioSegment\n",
    "from io import BytesIO\n",
    "\n",
    "import time\n",
    "\n",
    "\n",
    "# Constants\n",
    "FORMAT = pyaudio.paInt16\n",
    "CHANNELS = 1\n",
    "RATE = 16000\n",
    "CHUNK_DURATION_MS = 30  # milliseconds\n",
    "CHUNK_SIZE = int(RATE * CHUNK_DURATION_MS / 1000)  # samples per chunk\n",
    "SILENCE_THRESHOLD = 50  # adjust this threshold according to your environment\n",
    "TARGET_DURATION_MS = 700  # milliseconds\n",
    "\n",
    "def is_silence(chunk):\n",
    "    return not vad.is_speech(chunk, RATE)\n",
    "\n",
    "# Initialize PyAudio\n",
    "audio = pyaudio.PyAudio()\n",
    "\n",
    "# Initialize WebRTC VAD\n",
    "vad = webrtcvad.Vad()\n",
    "vad.set_mode(3)  # Aggressive mode for better voice detection\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Listening...\n",
      "हाई सो नॉईस किया या नहीं किया\n",
      "\n",
      "0.0008525848388671875\n",
      "2.0941884517669678\n",
      "Check if the noise went away or not.\n",
      "\n",
      "0.0009522438049316406\n",
      "1.3455562591552734\n",
      "மூடி பார்த்துக்கொண்டு வருகிறேன்\n",
      "\n",
      "0.0008170604705810547\n",
      "1.2093002796173096\n",
      "Stopping...\n"
     ]
    }
   ],
   "source": [
    "# Open stream\n",
    "stream = audio.open(format=FORMAT, channels=CHANNELS, rate=RATE,\n",
    "                     input=True, frames_per_buffer=CHUNK_SIZE)\n",
    "\n",
    "print(\"Listening...\")\n",
    "\n",
    "try:\n",
    "    accumulated_data = b\"\"\n",
    "    accumulated_silence = 0\n",
    "    while True:\n",
    "        chunk = stream.read(CHUNK_SIZE) #read mic data\n",
    "        is_silent = is_silence(chunk)\n",
    "        if is_silent:\n",
    "            accumulated_silence += CHUNK_DURATION_MS\n",
    "            if accumulated_silence >= TARGET_DURATION_MS:\n",
    "\n",
    "                if accumulated_data != b\"\" :\n",
    "                    t0 = time.time()\n",
    "\n",
    "                    audio_stream = BytesIO(accumulated_data)\n",
    "\n",
    "                    audio_segment = AudioSegment.from_file(audio_stream, format='raw',\n",
    "                                        frame_rate=16000, channels=1, sample_width=2)\n",
    "                    audio_segment.export('output.wav', format='wav')\n",
    "                    audio_file = open(\"output.wav\", \"rb\")\n",
    "                    t1 = time.time()\n",
    "\n",
    "                    transcription = client.audio.transcriptions.create(\n",
    "                        model=\"whisper-1\", \n",
    "                        file=audio_file, \n",
    "                        response_format=\"text\"\n",
    "                        )\n",
    "                    print(transcription)\n",
    "                    t2 = time.time()\n",
    "\n",
    "                    print(t1-t0)\n",
    "                    print(t2-t1)\n",
    "                # segments, info = model.transcribe('output.wav', beam_size=5)\n",
    "                # print(segments)\n",
    "\n",
    "                    accumulated_data=b\"\"\n",
    "\n",
    "\n",
    "                #print(\"No sound detected for {} milliseconds\".format(accumulated_silence))\n",
    "                accumulated_silence = 0  # Reset accumulated silence after printing message\n",
    "        else:\n",
    "            accumulated_data += chunk\n",
    "            accumulated_silence = 0  # Reset accumulated silence if sound is detected\n",
    "except KeyboardInterrupt:\n",
    "    pass\n",
    "\n",
    "# Cleanup\n",
    "print(\"Stopping...\")\n",
    "stream.stop_stream()\n",
    "stream.close()\n",
    "audio.terminate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from io import BytesIO\n",
    "import base64\n",
    "import pyautogui\n",
    "from openai import OpenAI \n",
    "\n",
    "photo = pyautogui.screenshot()\n",
    "\n",
    "output = BytesIO()\n",
    "photo.save(output, format='PNG')\n",
    "im_data = output.getvalue()\n",
    "\n",
    "image_data = base64.b64encode(im_data).decode(\"utf-8\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The image is a screenshot of a Visual Studio Code (VS Code) window. The window is displaying a Python script within a Jupyter Notebook file named \"final-asr-vad.ipynb\". The script appears to be related to taking a screenshot using the `pyautogui` library, encoding the image in base64, and then using the OpenAI API to generate a response.\n",
      "\n",
      "The VS Code interface includes the following elements:\n",
      "- The top menu bar with options like File, Edit, Selection, View, Go, Run, Terminal, and Help.\n",
      "- The notebook file name \"final-asr-vad.ipynb\" is visible at the top, along with the name of the project \"homegrownASR\".\n",
      "- The left sidebar contains icons for various tools and extensions, such as Explorer, Search, Source Control, Run and Debug, Extensions, and others.\n",
      "- The main editor area is split into two code cells. The first cell contains code for taking a screenshot and encoding it in base64. The second cell, titled \"Gpt\", contains code for sending a request to the OpenAI API to describe the image.\n",
      "\n",
      "The code in the first cell includes:\n",
      "```python\n",
      "from io import BytesIO\n",
      "import base64\n",
      "import pyautogui\n",
      "from openai import OpenAI\n",
      "\n",
      "photo = pyautogui.screenshot()\n",
      "\n",
      "output = BytesIO()\n",
      "photo.save(output, format=\"PNG\")\n",
      "im_data = output.getvalue()\n",
      "\n",
      "image_data = base64.b64encode(im_data).decode(\"utf-8\")\n",
      "```\n",
      "\n",
      "The code in the second cell includes:\n",
      "```python\n",
      "from io import BytesIO\n",
      "import base64\n",
      "import pyautogui\n",
      "from openai import OpenAI\n",
      "\n",
      "photo = pyautogui.screenshot()\n",
      "\n",
      "output = BytesIO()\n",
      "photo.save(output, format=\"PNG\")\n",
      "im_data = output.getvalue()\n",
      "\n",
      "image_data = base64.b64encode(im_data).decode(\"utf-8\")\n",
      "\n",
      "QUESTION=\"Describe the image\"\n",
      "MODEL=\"gpt-4.0\"\n",
      "client = OpenAI()\n",
      "\n",
      "response = client.chat.completions.create(\n",
      "    model=MODEL,\n",
      "    messages=[\n",
      "        {\"role\": \"system\", \"content\": \"You are a helpful assistant\"},\n",
      "        {\"role\": \"user\", \"content\": QUESTION},\n",
      "        {\"role\": \"user\", \"content\": image_data}\n",
      "    ]\n",
      ")\n",
      "```\n",
      "\n",
      "The VS Code interface also shows the Python environment being used (.conda (Python 3.11.9)) and some additional icons for running and debugging the code.\n"
     ]
    }
   ],
   "source": [
    "from io import BytesIO\n",
    "import base64\n",
    "import pyautogui\n",
    "from openai import OpenAI \n",
    "\n",
    "photo = pyautogui.screenshot()\n",
    "\n",
    "output = BytesIO()\n",
    "photo.save(output, format='PNG')\n",
    "im_data = output.getvalue()\n",
    "\n",
    "image_data = base64.b64encode(im_data).decode(\"utf-8\")\n",
    "\n",
    "\n",
    "\n",
    "QUESTION=\"Describe the image\"\n",
    "MODEL=\"gpt-4o\"\n",
    "client = OpenAI()\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=MODEL,\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant. Ignore the left side of the image and answer about the question asked on the right side of the image\"},\n",
    "        {\"role\": \"user\", \"content\": [\n",
    "            {\"type\": \"text\", \"text\": QUESTION},\n",
    "            {\"type\": \"image_url\", \"image_url\": {\n",
    "                \"url\": f\"data:image/png;base64,{image_data}\"}\n",
    "            }\n",
    "        ]}\n",
    "    ],\n",
    "    temperature=0.0,\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialization pyaudio for microphone and Webrtc vad\n",
    "\n",
    "# pyaudio audio\n",
    "import pyaudio\n",
    "import webrtcvad\n",
    "import numpy as np\n",
    "from pydub import AudioSegment\n",
    "from io import BytesIO\n",
    "\n",
    "# Image\n",
    "import base64\n",
    "import pyautogui\n",
    "\n",
    "#GPT and Transcribe\n",
    "from openai import OpenAI \n",
    "\n",
    "import time\n",
    "\n",
    "\n",
    "# Constants\n",
    "FORMAT = pyaudio.paInt16\n",
    "CHANNELS = 1\n",
    "RATE = 16000\n",
    "CHUNK_DURATION_MS = 30  # milliseconds\n",
    "CHUNK_SIZE = int(RATE * CHUNK_DURATION_MS / 1000)  # samples per chunk\n",
    "SILENCE_THRESHOLD = 50  # adjust this threshold according to your environment\n",
    "TARGET_DURATION_MS = 700  # milliseconds\n",
    "\n",
    "def is_silence(chunk):\n",
    "    return not vad.is_speech(chunk, RATE)\n",
    "\n",
    "# Initialize PyAudio\n",
    "audio = pyaudio.PyAudio()\n",
    "\n",
    "# Initialize WebRTC VAD\n",
    "vad = webrtcvad.Vad()\n",
    "vad.set_mode(3)  # Aggressive mode for better voice detection\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from io import BytesIO\n",
    "import base64\n",
    "import pyautogui\n",
    "from openai import OpenAI \n",
    "\n",
    "\n",
    "MODEL=\"gpt-4o\"\n",
    "client = OpenAI()\n",
    "\n",
    "\n",
    "\n",
    "# Open stream\n",
    "stream = audio.open(format=FORMAT, channels=CHANNELS, rate=RATE,\n",
    "                     input=True, frames_per_buffer=CHUNK_SIZE)\n",
    "\n",
    "print(\"Listening...\")\n",
    "\n",
    "try:\n",
    "    accumulated_data = b\"\"\n",
    "    accumulated_silence = 0\n",
    "    while True:\n",
    "        chunk = stream.read(CHUNK_SIZE) #read mic data\n",
    "        is_silent = is_silence(chunk)\n",
    "        if is_silent:\n",
    "            accumulated_silence += CHUNK_DURATION_MS\n",
    "            if accumulated_silence >= TARGET_DURATION_MS:\n",
    "\n",
    "                if accumulated_data != b\"\" :\n",
    "                    t0 = time.time()\n",
    "                    \n",
    "                    #audio\n",
    "                    audio_stream = BytesIO(accumulated_data)\n",
    "                    audio_segment = AudioSegment.from_file(audio_stream, format='raw',\n",
    "                                        frame_rate=16000, channels=1, sample_width=2)\n",
    "                    audio_segment.export('output.wav', format='wav')\n",
    "                    audio_file = open(\"output.wav\", \"rb\")\n",
    "                    t1 = time.time()\n",
    "\n",
    "                    #openai Transcribe\n",
    "                    transcription = client.audio.transcriptions.create(\n",
    "                        model=\"whisper-1\", \n",
    "                        file=audio_file, \n",
    "                        response_format=\"text\"\n",
    "                        )\n",
    "                    print(\"Question = \",transcription)\n",
    "                    t2 = time.time()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                    # Take Screen Shots\n",
    "                    photo = pyautogui.screenshot()\n",
    "                    output = BytesIO()\n",
    "                    photo.save(output, format='PNG')\n",
    "                    im_data = output.getvalue()\n",
    "                    image_data = base64.b64encode(im_data).decode(\"utf-8\")\n",
    "\n",
    "                    t3= time.time()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                    QUESTION=transcription\n",
    "\n",
    "                    response = client.chat.completions.create(\n",
    "                        model=MODEL,\n",
    "                        messages=[\n",
    "                            {\"role\": \"system\", \"content\": \"\"\"You are a helpful assistant. Ignore the left side of the image and use ONLY right side of the image. \n",
    "                             HELP the student to THINK. SEE if he is solving the question correctly.\n",
    "                             Give concise answers not more than 20 words long. Use the image as a reference to check if they are going the right way\"\"\"},\n",
    "                            {\"role\": \"user\", \"content\": [\n",
    "                                {\"type\": \"text\", \"text\": QUESTION},\n",
    "                                {\"type\": \"image_url\", \"image_url\": {\n",
    "                                    \"url\": f\"data:image/png;base64,{image_data}\"}\n",
    "                                }\n",
    "                            ]}\n",
    "                        ],\n",
    "                        temperature=0.0,\n",
    "                    )\n",
    "\n",
    "                    t4= time.time()\n",
    "\n",
    "                    \n",
    "\n",
    "\n",
    "                    print(\"GPT answer= \",response.choices[0].message.content)\n",
    "                # segments, info = model.transcribe('output.wav', beam_size=5)\n",
    "                # print(segments)\n",
    "                    print(\"\\n Audio conversion\",t1-t0)\n",
    "                    print(\"Take Image= \",t3-t2)\n",
    "                    print(\"Transcription Time= \",t2-t1)\n",
    "                    print(\"Time taken by GPT= \",t4-t3)\n",
    "                    print(\"Total Time = \",t4-t0 )\n",
    "\n",
    "                    accumulated_data=b\"\"\n",
    "\n",
    "\n",
    "                #print(\"No sound detected for {} milliseconds\".format(accumulated_silence))\n",
    "                accumulated_silence = 0  # Reset accumulated silence after printing message\n",
    "        else:\n",
    "            accumulated_data += chunk\n",
    "            accumulated_silence = 0  # Reset accumulated silence if sound is detected\n",
    "except KeyboardInterrupt:\n",
    "    pass\n",
    "\n",
    "# Cleanup\n",
    "print(\"Stopping...\")\n",
    "stream.stop_stream()\n",
    "stream.close()\n",
    "audio.terminate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
